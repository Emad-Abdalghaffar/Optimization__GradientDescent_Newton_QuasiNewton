#### Author: Emad Abdalghaffar
#### 2022 _ Information Technology Institute (ITI) _ AI and Machine Learning program
<br><br>

#### Problem (1)
####          Gradient Descent optimizer in Linear Regression problems
<br>
Provided are codes of the gradient descent variant optimizers:

- Batch/Vanilla GD  (Single Variable Function)<br>

- Batch/Vanilla GD  (Multivariant Function)<br>

- Stochastic GD  (Single Variable Function)<br>

- Mini Batch GD  (Single Variable Function)<br>

- Momentum-based GD  (Batch & Single Variable Function)<br>

- NAG  (Batch & Single)<br>

- Adam GD  (Batch & Multivariant Function)<br>

- Adam GD  (Mini Batch & Multivariant Function)<br><br>




#### Problem (2)
####         Newton's & Quasi-Newton (BFGS) Optimization Algorithms
####         (For Single and Multivariant Quadratic Functions)
<br>
Provided are codes of the Newton's and Quasi-Newton optimizers:

- Secant and Newton Root-Finding Algorithms<br>

- Gradient Descent and Newton Optimization Algorithms    (Quadratic Single-Variable function)<br>

- Gradient Descent and Newton Optimization Algorithms    (Quadratic Multivariant functions)<br>

- Quasi Newton (BFGS) Optimization Algorithm    (Quadratic Multivariant functions)<br><br>




#### Problem (3)
####         Comparing different optimizers in a Linear Regression problem
<br>
Provided are codes with the following optimizers:

- GD optimizer (MSE loss function)<br>

- Quasi Newton (BFGS) optimizer (MSE loss function)<br>

- Ordinary Least Square optimizer by sklearn (RMSE loss function)<br>
