## Author: Emad Abdalghaffar
## 2022 - Information Technology Institute (ITI)
<br>

### Problem (1)
####          Gradient Descent optimization of the cost function (MSE) in Linear Regression problems
<br>
Provided is the implementation of the gradient descent algorithm different variants:

- Batch/Vanilla GD  (Single Variable Function)<br>

- Batch/Vanilla GD  (Multivariant Function)<br>

- Stochastic GD  (Single Variable Function)<br>

- Mini Batch GD  (Single Variable Function)<br>

- Momentum-based GD  (Batch & Single Variable Function)<br>

- NAG  (Batch & Single)<br>

- Adam GD  (Batch & Multivariant Function)<br>

- Adam GD  (Mini Batch & Multivariant Function)<br><br>




### Problem (2)
####         Newton's & Quasi-Newton (BFGS) Optimization Algorithms
####         (For Single and Multivariant Quadratic Functions)
<br>
Provided is the implementation of the following algorithms:

- Secant and Newton Root-Finding Algorithms<br>

- Gradient Descent and Newton Optimization Algorithms    (Quadratic Single-Variable function)<br>

- Gradient Descent and Newton Optimization Algorithms    (Quadratic Multivariant functions)<br>

- Quasi Newton (BFGS) Optimization Algorithm    (Quadratic Multivariant functions)<br>
